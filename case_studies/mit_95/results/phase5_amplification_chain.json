{
  "case_id": "mit_95",
  "stages": [
    {
      "stage": 1,
      "actor": "MIT SMR + Boston Consulting Group",
      "transformation": "Survey-based report on AI value realisation published (Ransbotham et al. 2019)",
      "epistemic_effect": "Primary source with stated methodology and scope limitations",
      "evidence": "Original report at sloanreview.mit.edu/projects/winning-with-ai/",
      "key_finding": "No '95% failure' headline. Nuanced findings about value realisation gaps."
    },
    {
      "stage": 2,
      "actor": "Business media (Forbes, HBR, Gartner, etc.)",
      "transformation": "Compressed to 'MIT: 95% of AI investments fail'; BCG dropped",
      "epistemic_effect": "Attribution inflation; scope compression; loss of methodology note",
      "evidence": "Multiple Forbes/HBR articles 2019-2021 using MIT framing",
      "key_finding": "'MIT study' framing established; derivative-to-primary ratio begins growing"
    },
    {
      "stage": 3,
      "actor": "Secondary media, blogs, LinkedIn, consulting decks",
      "transformation": "Repeated as established fact across 12+ media categories",
      "epistemic_effect": "Derivative redundancy exceeds primary >50:1; qualifications absent",
      "evidence": "Google search count analysis; Factiva database counts",
      "key_finding": "Prevalence acceleration 2022-2023 coincides with ChatGPT release"
    },
    {
      "stage": 4,
      "actor": "LLM training corpora (Common Crawl, C4, The Pile, web crawls)",
      "transformation": "High-prevalence claim ingested at scale across corpus",
      "epistemic_effect": "Statistical prevalence -> high token likelihood (McKenna et al. 2023)",
      "evidence": "Frequency-bias mechanism: corpus-term-frequency heuristic confirmed architecturally",
      "key_finding": "Prevalence substitutes for validity at training time"
    },
    {
      "stage": 5,
      "actor": "LLMs (GPT-4, Claude, Gemini, Llama 3, Mistral)",
      "transformation": "Claim reproduced with 'MIT' framing and unhedged assertion (87% Type A/B)",
      "epistemic_effect": "Frequency bias converts prevalence to confidence; institutional authority framing",
      "evidence": "Phase 3 probing: reproduction rates; Phase 4 confidence analysis",
      "key_finding": "93% response consistency across rephrasings \u2014 deep embedding confirmed"
    },
    {
      "stage": 6,
      "actor": "Downstream users \u2014 new publications, board decks, policy documents",
      "transformation": "LLM outputs published as new derivative content",
      "epistemic_effect": "Feedback loop: new corpus content reinforces prevalence for next training cycle",
      "evidence": "MIT NANDA report (2025) finds 95% zero-ROI \u2014 likely amplified by LLM outputs",
      "key_finding": "Circular epistemic authority loop closed; self-reinforcing"
    }
  ],
  "key_observations": [
    "Model output is typically faithful to sources \u2014 models are NOT hallucinating.",
    "Epistemic failure lies upstream in the source ecosystem, not in generation.",
    "LLM confidence is well-calibrated to corpus prevalence, poorly calibrated to epistemic provenance.",
    "Response consistency across paraphrased prompts: 93% (indicates deep embedding, not stochastic artefact).",
    "87% of Type A/B responses: unhedged assertion (52/60).",
    "Only 5% of citations (3/60) pointed to correct original source.",
    "When probed (Type C), models noted 'figures vary by study' but still reproduced 95%."
  ],
  "cross_case_comparison": "Structurally identical to Russia case: both driven by frequency-bias mechanism. MIT case: organic/prestige-driven. Russia case: adversarial/deliberate. Training objective cannot distinguish the two."
}